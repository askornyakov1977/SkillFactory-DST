{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostRegressor\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "import funcs \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем формат логирования\n",
    "logging.basicConfig(\n",
    "   format=\"%(levelname)s: %(asctime)s: %(message)s\",\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cоздаем лог-файл\n",
    "logger = funcs.get_logger(path=\"logs/\", file=\"model.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-09-11 06:11:57,462: Data file: api01.csv\n",
      "INFO: 2024-09-11 06:11:57,467: Data shape: (40109, 4)\n",
      "INFO: 2024-09-11 06:11:57,479: Data head: \n",
      "         date      time          id  value\n",
      "0  12.08.2024  14:32:24  1723462344    912\n",
      "1  12.08.2024  14:30:40  1723462240    657\n",
      "2  12.08.2024  14:29:17  1723462157    872\n",
      "3  12.08.2024  14:28:14  1723462094    500\n",
      "4  12.08.2024  14:27:04  1723462024    852\n"
     ]
    }
   ],
   "source": [
    "# Считаем файл-csv импортированный из БД мониторинга в датафрейм\n",
    "file_name = 'api01.csv'\n",
    "data = pd.read_csv(file_name, sep=' ')\n",
    "logger.info(f\"Data file: {file_name}\")\n",
    "logger.info(f\"Data shape: {data.shape}\")\n",
    "logger.info(f\"Data head: \\n{data.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-09-11 06:12:01,997: Number of entries before filling in gaps: 1440\n",
      "INFO: 2024-09-11 06:12:02,015: Number of entries after filling in the blanks: 1440\n",
      "INFO: 2024-09-11 06:12:02,036: Basic statistical characteristics of end-to-end values of a time series: \n",
      "        lower_bound       median  upper_bound\n",
      "count  1440.000000  1440.000000  1440.000000\n",
      "mean    300.369132   343.182986   478.380243\n",
      "std      11.924069    31.102221   139.257811\n",
      "min     255.800000   287.000000   314.850000\n",
      "25%     293.000000   322.000000   368.000000\n",
      "50%     301.000000   336.500000   429.875000\n",
      "75%     307.900000   357.500000   559.625000\n",
      "max     351.050000   511.000000  1541.250000\n",
      "INFO: 2024-09-11 06:12:03,994: Transformed data: \n",
      "                       y\n",
      "date_time               \n",
      "2024-07-13 14:33:00  785\n",
      "2024-07-13 14:34:00  547\n",
      "2024-07-13 14:35:00  448\n",
      "2024-07-13 14:36:00  415\n",
      "2024-07-13 14:38:00  381\n",
      "INFO: 2024-09-11 06:12:03,997: Number of records before filling gaps and merging duplicates: 40109\n",
      "INFO: 2024-09-11 06:12:04,010: Number of records after filling gaps and merging duplicates: 43200\n",
      "INFO: 2024-09-11 06:12:30,175: Data with a new target feature: \n",
      "                   y       y_clean\n",
      "count  35971.000000  43200.000000\n",
      "mean     421.176364    350.381806\n",
      "std      527.940351     56.343374\n",
      "min        0.000000    262.000000\n",
      "25%      308.000000    319.000000\n",
      "50%      334.000000    335.000000\n",
      "75%      394.000000    361.000000\n",
      "max    25352.000000   1034.000000\n"
     ]
    }
   ],
   "source": [
    "# Получим выборку с очищенными данными\n",
    "data_cleaned = funcs.get_data_cleaned(data, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделям исходные признаки и целевой признак в отдельные наборы данных\n",
    "X = data_cleaned.drop('y', axis=1)\n",
    "y = data_cleaned.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-09-11 06:12:42,177: Shape before transform: (43200, 0)\n",
      "INFO: 2024-09-11 06:12:42,180: Shape after transform: (43200, 85)\n"
     ]
    }
   ],
   "source": [
    "# Трансформируем исходные данные \n",
    "X_transformed = funcs.get_data_transformed(X)\n",
    "logger.info('Shape before transform: {}'.format(X.shape))\n",
    "logger.info('Shape after transform: {}'.format(X_transformed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-09-11 06:12:49,181: Training sample size: (41760, 85), begin: 2024-07-13 14:33:00, end: 2024-08-11 14:32:00\n",
      "INFO: 2024-09-11 06:12:49,186: Test sample size: (1440, 85), begin: 2024-08-11 14:33:00, end: 2024-08-12 14:32:00\n"
     ]
    }
   ],
   "source": [
    "# Вычислим момент времени для разделения датачета на тренировочную и тестовую выборки\n",
    "split_datetime = X_transformed.index.max() - pd.to_timedelta(1, 'd')\n",
    "# Создадим тренировочную и тестовую выборки\n",
    "X_train = X_transformed[X_transformed.index<=split_datetime]\n",
    "y_train = y[y.index<=split_datetime]\n",
    "X_test = X_transformed[X_transformed.index>split_datetime]\n",
    "y_test = y[y.index>split_datetime]\n",
    "logger.info(f'Training sample size: {X_train.shape}, begin: {X_train.index.min()}, end: {X_train.index.max()}')\n",
    "logger.info(f'Test sample size: {X_test.shape}, begin: {X_test.index.min()}, end: {X_test.index.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполним масштабирование исходных признаков\n",
    "scaler = MinMaxScaler()\n",
    "# Подгоняем параметры стандартизатора\n",
    "scaler.fit(X_train)\n",
    "# Производим стандартизацию тренировочной выборки\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train),\n",
    "                              columns=X_train.columns, index=X_train.index)\n",
    "# Производим стандартизацию тестовой выборки\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test),\n",
    "                              columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявим вспомогательную функцию по валидации данных временного ряда.\n",
    "def timeseries_validation(model, X_train, y_train, n_splits=3):\n",
    "  tss = TimeSeriesSplit(n_splits=n_splits, test_size=1440)\n",
    "  train_test_groups = tss.split(X_train)\n",
    "  metrics = []\n",
    "  for train_index, test_index in train_test_groups:\n",
    "    # обучаем модель\n",
    "    model.fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "    metrics.append(mean_absolute_percentage_error(y_train.iloc[test_index],\n",
    "                                          model.predict(X_train.iloc[test_index])))\n",
    "  return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявим целевую функцию для поиска гиперпараметров, \n",
    "# происходит инициализация модели по входящему в функцию параметру, \n",
    "# далее выполняется валидация на основе обучающей выборки, и выдается средняя метрика.\n",
    "def optuna_CatBoostRegressor(trial):\n",
    "            # задаем пространства поиска гиперпараметров\n",
    "            model_params =  {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1500),\n",
    "                #'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.2, log=True),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "                #\"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "                #\"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n",
    "                \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 64),\n",
    "                \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=False),\n",
    "                'random_state': trial.suggest_int(\"random_state\", 42, 42),\n",
    "                'verbose': trial.suggest_int(\"verbose\", 0, 0)\n",
    "            }\n",
    "            # создаем модель\n",
    "            model = CatBoostRegressor(**model_params)\n",
    "            # запускаем валидацию обучающей выбоки\n",
    "            score = timeseries_validation(model, X_train_scaled, y_train)\n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-11 06:13:11,633] A new study created in memory with name: CatBoostRegressor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Поиск оптимальных гиперпараметров:  CatBoostRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-11 06:14:37,888] Trial 0 finished with value: 0.06928951915823804 and parameters: {'n_estimators': 624, 'depth': 12, 'min_data_in_leaf': 48, 'colsample_bylevel': 0.0638792635777333, 'random_state': 42, 'verbose': 0}. Best is trial 0 with value: 0.06928951915823804.\n",
      "INFO: 2024-09-11 06:14:37,892: Optimal hyperparameters for the CatBoostRegressor model found: {'n_estimators': 624, 'depth': 12, 'min_data_in_leaf': 48, 'colsample_bylevel': 0.0638792635777333, 'random_state': 42, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Запустим поиск оптимальных гиперпараметров модели\n",
    "sampler = TPESampler(seed=42)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "print('Поиск оптимальных гиперпараметров: ', 'CatBoostRegressor')\n",
    "study = optuna.create_study(direction='minimize', study_name='CatBoostRegressor', sampler=sampler)\n",
    "study.optimize(optuna_CatBoostRegressor, n_trials=1)\n",
    "# Сохраним оптимальные гиперпараметры модели в структуру моделей\n",
    "best_params = study.best_trial.params\n",
    "logger.info(f'Optimal hyperparameters for the CatBoostRegressor model found: {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём пайплайн, который включает нормализацию, отбор признаков и обучение модели\n",
    "pipe = Pipeline([ \n",
    "  ('Scaling', MinMaxScaler()),\n",
    "  ('CatBoostRegressor', CatBoostRegressor(**best_params))\n",
    "  ])\n",
    "# Обучаем пайплайн\n",
    "pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявим функцию по выводу метрик\n",
    "def get_metrics(data_true, data_pred):\n",
    "  metric_funcs = {r2_score: 'R2', mean_absolute_percentage_error: 'MAPE',\n",
    "                  mean_absolute_error: 'MAE', mean_squared_error: 'MSE'}\n",
    "  return [(metric_funcs[func], func(data_true, data_pred)) for func in metric_funcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-09-11 06:15:20,278: Learning metrics: \n",
      "[('R2', 0.4932229687487738), ('MAPE', 0.059861532568902705), ('MAE', 22.904933627853936), ('MSE', 1599.6877476900324)]\n",
      "INFO: 2024-09-11 06:15:20,353: Test metrics: \n",
      "[('R2', 0.23206125245924125), ('MAPE', 0.05930073315435337), ('MAE', 24.621374863129024), ('MSE', 2836.484668333923)]\n"
     ]
    }
   ],
   "source": [
    "# Сделаем предсказание обучающей выборки\n",
    "y_pred_train = pipe.predict(X_train)\n",
    "# Сохраним обучающие метрики в таблице итоговых результатов\n",
    "metrics = get_metrics(y_train, y_pred_train)\n",
    "logger.info(f\"Learning metrics: \\n{metrics}\")\n",
    "# Сделаем предсказание тестовой выборки\n",
    "y_pred_test = pipe.predict(X_test)\n",
    "# Сохраним тестовые метрики в таблице итоговых результатов\n",
    "metrics = get_metrics(y_test, y_pred_test)\n",
    "logger.info(f\"Test metrics: \\n{metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483097.7193900346"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-09-11 05:27:22,091: The serialized pipeline is written to a file: pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "# Сериализуем pipeline и записываем результат в файл\n",
    "file_name = 'pipeline.pkl'\n",
    "with open(file_name, 'wb') as output:\n",
    "    pickle.dump(pipe, output)\n",
    "logger.info(f'The serialized pipeline is written to a file: {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Десериализуем pipeline из файла\n",
    "with open('pipeline.pkl', 'rb') as pkl_file:\n",
    "    loaded_pipe = pickle.load(pkl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
